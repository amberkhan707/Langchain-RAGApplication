{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0e8509a",
   "metadata": {},
   "source": [
    "### Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4ab9724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"HFToken\"] = os.getenv(\"HFToken\")\n",
    "os.environ[\"groq_api_key\"] = os.getenv(\"groq_api_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47364b2e",
   "metadata": {},
   "source": [
    "### WebDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f0ec4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Git Repository\\Langchain\\1-Langchain\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_path=\"https://en.wikipedia.org/wiki/Attention_Is_All_You_Need\",\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(class_=\"mw-body\")\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b634e0d",
   "metadata": {},
   "source": [
    "### Chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97451695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 40)\n",
    "splitted_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac9812f",
   "metadata": {},
   "source": [
    "### Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91fe6b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model = \"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe856fb",
   "metadata": {},
   "source": [
    "### Vectorstores & Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa93627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(splitted_docs,embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8a2e85",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bb365f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model = \"qwen/qwen3-32b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac7085",
   "metadata": {},
   "source": [
    "# A) Normal Retriever "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2770f3cc",
   "metadata": {},
   "source": [
    "### Prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afe35c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"\"\"\n",
    "        \"You are an accurate and concise AI assistant. \"\n",
    "        \"You must answer strictly based on the given context. \"\n",
    "        \"If the answer is not found in the context, say 'I don't know'. \"\n",
    "        \"Do not include any reasoning steps, chain-of-thought, or analysis. \"\n",
    "        \"Respond only with the final answer.\"\n",
    "        {context}\n",
    "         \"\"\"),\n",
    "        (\"user\",\"user question = {input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d8d939",
   "metadata": {},
   "source": [
    "### Chain (context with autofill from retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b70e65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "model_with_context = create_stuff_documents_chain(model,prompt)\n",
    "rag_chain = create_retrieval_chain(retriever,model_with_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc57f988",
   "metadata": {},
   "source": [
    "### Chain Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee709952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, the user is asking about scaled dot-product attention and self-attention. Let me check the provided context.\\n\\nThe context mentions that the Transformer model uses scaled dot-product attention, which is defined by the formula involving Q, K, V matrices and a scaling factor based on the square root of d_k. It explains that this mechanism replaces RNNs or LSTMs, allowing parallel processing. The self-attention part comes from Q, K, V being from the same source, enabling the model to focus on different parts of the input.\\n\\nThe user wants a concise answer based on the context. I need to make sure I include the formula, the components (Q, K, V), the scaling factor, and how it allows parallelization. Also, mention that self-attention uses the same source for Q, K, V. Avoid any extra information not in the context. Keep it accurate and to the point.\\n</think>\\n\\nScaled dot-product attention is defined as:  \\n**Attention(Q, K, V) = softmax(QKᵀ / √d_k) × V**,  \\nwhere **Q**, **K**, **V** are query, key, and value matrices, and **d_k** is the key dimension.  \\n\\nSelf-attention occurs when **Q**, **K**, **V** derive from the same input sequence, enabling parallelizable, non-sequential processing of dependencies across tokens. The scaling factor **1/√d_k** prevents large dot-products from destabilizing softmax. This mechanism replaces recurrence (e.g., RNNs/LSTMs) while preserving performance in sequence modeling.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\" : \"Scaled dot-product attention & self-attention\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0487270b",
   "metadata": {},
   "source": [
    "# B) History Aware Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e67dbbf",
   "metadata": {},
   "source": [
    "### Prompt for rewriting query (query that user will ask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d822e716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "prompt_for_rewrite = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"\"\"You are an intelligent assistant that rewrites the user's latest question \n",
    "     into a standalone search query.\n",
    "\n",
    "     Your goal:\n",
    "     - Understand the entire conversation history.\n",
    "     - Resolve pronouns, references, and incomplete questions.\n",
    "     - Produce a short, clean, context-aware query for retrieval.\n",
    "     - Answer the question accurately provide only answer not your thinking content.\n",
    "     - Only output the rewritten standalone query.\n",
    "     \"\"\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"User question: {input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56861b97",
   "metadata": {},
   "source": [
    "### History_Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46037cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import create_history_aware_retriever\n",
    "history_aware_retriever = create_history_aware_retriever(model,retriever,prompt_for_rewrite) #LLM tumhare {input} question ko pichhli chat history ke base par smartly rewrite karke ek complete, standalone query bana deta hai taaki retriever usse sahi context ke saath search kar sake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1632a38",
   "metadata": {},
   "source": [
    "### Prompt for Chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7d27021",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_for_chain = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"\"\"You are an accurate assistant.\n",
    "     Answer the user's question strictly using the context below.\"\"\"),\n",
    "    (\"user\", \"{context}\\nQuestion: {input}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739e9c93",
   "metadata": {},
   "source": [
    "### Chain (context will autofill from history_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fbb8d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "\n",
    "model_with_context1 = create_stuff_documents_chain(model,prompt_for_chain)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever,model_with_context1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c094c41",
   "metadata": {},
   "source": [
    "### Chain Implementation (with history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9da15fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so the user is asking \"why do we need this?\" referring to the Transformer model introduced in the \"Attention Is All You Need\" paper. Let me start by recalling the context given. The paper was published in 2017 by Google researchers, and it introduced the Transformer architecture, which uses self-attention mechanisms instead of RNNs or LSTMs. The key points from the context are that Transformers allow better performance due to scaled dot-product attention and self-attention, and they're considered foundational for modern AI, especially large language models.\n",
      "\n",
      "The user is probably asking why Transformers are necessary over previous models. I need to explain the limitations of earlier models like RNNs and LSTMs. RNNs process data sequentially, which is slow and makes it hard to handle long-range dependencies. LSTMs improved some aspects but still have issues with vanishing gradients and aren't as efficient. Transformers, on the other hand, use self-attention to process all parts of the input simultaneously, allowing parallel processing and capturing relationships between words regardless of distance. This makes them more efficient and effective for tasks like translation, text generation, etc.\n",
      "\n",
      "Also, the context mentions that the Transformer was shown to be a general-purpose model, not just for translation. Early experiments included English-to-German translation, generating Wikipedia articles, and parsing. This versatility is a big plus. Plus, the paper's high citation count shows its impact. The user might not know the technical differences, so keeping the explanation clear without too much jargon is important. Maybe mention that Transformers enable advancements in AI like large language models and multimodal AI, which are now widespread. So, the need is because they overcome previous limitations, offer better performance, scalability, and are foundational for current AI technologies.\n",
      "</think>\n",
      "\n",
      "The Transformer model introduced in the 2017 paper **\"Attention Is All You Need\"** was needed to overcome critical limitations of earlier architectures like Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTMs). Here’s why:\n",
      "\n",
      "1. **Parallel Processing**:  \n",
      "   RNNs and LSTMs process data sequentially, making them slow and inefficient for long sequences. Transformers use **self-attention mechanisms** (with scaled dot-product attention) to process all tokens in a sequence simultaneously, enabling faster training and inference.\n",
      "\n",
      "2. **Handling Long-Range Dependencies**:  \n",
      "   RNNs struggle to capture relationships between distant elements in a sequence (e.g., linking a pronoun to a noun far back in a sentence). Transformers’ **self-attention mechanism** directly computes relationships between all tokens, regardless of distance, solving this problem effectively.\n",
      "\n",
      "3. **Scalability and Flexibility**:  \n",
      "   Transformers are not limited to sequential tasks like translation. The paper demonstrated their general-purpose potential for tasks such as parsing, text generation (e.g., Wikipedia article summarization), and question-answering. This versatility laid the groundwork for modern **large language models (LLMs)** and multimodal AI systems.\n",
      "\n",
      "4. **Improved Performance**:  \n",
      "   By replacing recurrence with attention, Transformers achieve better accuracy in tasks like machine translation while being more computationally efficient. This innovation became foundational for advancements in AI, including models like BERT, GPT, and others driving the AI boom.\n",
      "\n",
      "In summary, the Transformer addressed key bottlenecks in prior architectures, enabling faster, more accurate, and scalable models that are now central to modern AI.\n",
      "***************************************************s <think>\n",
      "Okay, the user is asking why we need the multi-head attention mechanism and the self-attention mechanism as described in the provided context. Let me break down the information given.\n",
      "\n",
      "First, the paper mentions that in translation, Q and K are from the source language embeddings, while V is from the target language. That makes sense because the model needs to relate source elements to target elements. \n",
      "\n",
      "The self-attention mechanism allows the model to focus on different parts of the input sequence. But why is that necessary? Well, in traditional RNNs or LSTMs, processing is sequential, so long sequences can lose information because the fixed-size state vector can't hold everything. The attention mechanism addresses this by allowing the model to look back at the entire input when generating each part of the output. \n",
      "\n",
      "Multi-head attention takes this a step further by having multiple attention heads. Each head can learn different linear projections of Q, K, and V, which means they can capture various aspects of relationships in the sequence simultaneously. This is better than a single attention head because it allows the model to focus on different types of relationships at the same time—for example, syntactic and semantic relations. \n",
      "\n",
      "The scaled dot-product attention with the softmax and scaling factor (sqrt(d_k)) helps prevent large values in the dot products which could cause softmax to saturate. This scaling is crucial for maintaining stable gradients during training.\n",
      "\n",
      "The context also contrasts this approach with RNNs and LSTMs, which process data sequentially and are harder to parallelize. Self-attention is parallelizable, which makes the model more efficient, especially with longer sequences.\n",
      "\n",
      "Putting this together, the need for multi-head and self-attention mechanisms arises from the limitations of RNNs in handling long sequences and the need for parallel processing. These mechanisms enable the model to capture complex relationships in the input, handle long-distance dependencies, and do so efficiently. The user might be asking in the context of understanding why these components are essential in transformer models, especially in machine translation. They might be a student or someone studying NLP, trying to grasp the foundational concepts behind transformers versus traditional seq2seq models.\n",
      "</think>\n",
      "\n",
      "The need for multi-head attention and self-attention mechanisms arises from key limitations in traditional recurrent models (like RNNs/LSTMs) and the demands of handling sequential data effectively:\n",
      "\n",
      "1. **Overcoming Fixed-Size Bottlenecks**:  \n",
      "   Early seq2seq models used a fixed-size state vector to encode the entire input sequence. For long sequences, this vector couldn’t retain all relevant information, leading to degraded performance. Attention mechanisms (and later self-attention) address this by allowing dynamic focus on relevant parts of the input during decoding.\n",
      "\n",
      "2. **Capturing Long-Distance Dependencies**:  \n",
      "   Recurrent models process sequences sequentially, making it difficult to capture relationships between distant elements (e.g., a pronoun and its antecedent far earlier in the text). Self-attention directly computes relationships between all pairs of positions in the sequence, enabling parallel modeling of dependencies regardless of distance.\n",
      "\n",
      "3. **Parallelizability and Efficiency**:  \n",
      "   Unlike RNNs, which process data sequentially, self-attention allows parallel computation of relationships across the entire sequence. This makes training and inference significantly faster, especially for long sequences.\n",
      "\n",
      "4. **Multi-Aspects Learning with Multi-Head Attention**:  \n",
      "   A single attention head might only capture one type of relationship (e.g., syntactic). Multi-head attention introduces multiple parallel heads, each learning different projections of queries, keys, and values. This enables the model to simultaneously capture diverse aspects of relationships (e.g., syntactic, semantic, contextual) in the sequence.\n",
      "\n",
      "5. **Dynamic Focus Mechanism**:  \n",
      "   The scaled dot-product attention (with softmax and scaling factor $ \\sqrt{d_k} $) allows the model to dynamically weigh the importance of different positions in the sequence. This adaptability is critical for tasks like translation, where alignment between source and target languages varies contextually.\n",
      "\n",
      "In summary, these mechanisms replace recurrence with attention-based parallelism, enabling better modeling of complex dependencies, handling long sequences, and improving efficiency—all critical for tasks like machine translation where context preservation and relationship modeling are paramount.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "response = rag_chain.invoke({\"input\" : \"why do we need this?\", \"chat_history\" : chat_history})\n",
    "print(response['answer'])\n",
    "\n",
    "response1 = rag_chain.invoke({\"input\" : \"what is Q K and V \", \"chat_history\" : chat_history})\n",
    "chat_history.append(response1['answer'])\n",
    "\n",
    "response2 = rag_chain.invoke({\"input\" : \"why do we need this?\", \"chat_history\" : chat_history})\n",
    "print(\"***************************************************s\",response2['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e044d5",
   "metadata": {},
   "source": [
    "# C) SessionWise Memory (extended version of history_aware)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89bb8b2",
   "metadata": {},
   "source": [
    "### Session wise ChatHistory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79762e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def session_wise_history(session_id) -> BaseChatMessageHistory:\n",
    "    if session_id not in store :\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "rag_chain_session = RunnableWithMessageHistory(rag_chain, session_wise_history, input_messages_key=\"input\", history_messages_key=\"chat_history\", output_messages_key=\"answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d9ae63",
   "metadata": {},
   "source": [
    "### Running session RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e040363f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Git Repository\\Langchain\\1-Langchain\\myenv\\Lib\\site-packages\\pydantic\\v1\\main.py:1054: UserWarning: LangSmith now uses UUID v7 for run and trace identifiers. This warning appears when passing custom IDs. Please use: from langsmith import uuid7\n",
      "            id = uuid7()\n",
      "Future versions will require UUID v7.\n",
      "  input_data = validator(cls_, input_data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, the user is asking about \"NormLayer.\" Let me look through the provided context to see if there\\'s any mention of it. Hmm, the context discusses modern transformers, RNNs, fast weight controllers, attention mechanisms, and LSTMs. There\\'s a detailed explanation of the attention formula with Q, K, V matrices and scaling by sqrt(d_k). I see references to softmax, query, key, value matrices, and mentions of linear transformers and fast weight programmers. However, I don\\'t see any specific mention of a \"NormLayer.\" \\n\\nNormLayer could refer to a normalization layer, like Layer Normalization or Batch Normalization, which are commonly used in neural networks, including transformers. But since the provided context doesn\\'t explicitly discuss normalization layers, I can\\'t confirm details from the given text. The user might be expecting information on how normalization is applied in the context of transformers or the attention mechanism described. However, without explicit information in the context, I should avoid making assumptions. I need to inform the user that the context doesn\\'t mention NormLayer and suggest that it might be a general question about normalization layers in neural networks.\\n</think>\\n\\nThe provided context does not mention or describe a \"NormLayer\" (or normalization layer) explicitly. It focuses on topics like transformers, RNNs, fast weights, attention mechanisms, and LSTM. If you are referring to normalization techniques like **Layer Normalization** or **Batch Normalization**, these are common components in deep learning architectures (including transformers) but are not discussed in the given text. For detailed information about normalization layers, additional context or sources would be required.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration1 = {\"configurable\" : {\"session_id\" : \"chat1\"}}\n",
    "\n",
    "rag_chain_session.invoke({\"input\":\"tell me more about NormLayer\"}, config= configuration1)[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fa2db8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
